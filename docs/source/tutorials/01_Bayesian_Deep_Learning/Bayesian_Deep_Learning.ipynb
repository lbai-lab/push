{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push Bayesian Deep Learning Tutorial\n",
    "## Introduction\n",
    "\n",
    "In this notebook we will introduce the concept of Bayesian Deep Learning and demonstrate its usage in Push by running a deep ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Posterior Predictive Distribution\n",
    "The goal of Bayesian Deep Learning (BDL) methods is to estimate the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y|x, D) = \\int p(y|x, w) p(w|D) \\, dw\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where y is an output, x is an input, w are parameters, and D is is the data. This integral is intractable and must be approximated. The typical method of approximation is Monte Carlo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y|x, D) = \\int p(y|x, w) p(w|D) \\, dw \\approx \\frac{1}{J} \\sum_{j} p(y|x, w_j), \\quad w_j \\sim p(w|D)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This integral is a Bayesian Model Average over J models and parameter settings. A Deep Ensemble is an average over a number of randomly initialized models, and thus is the primary justification for Deep Ensembles being a Bayesian Deep Learning method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating the Posterior Predictive Distribution\n",
    "\n",
    "In Bayesian Deep Learning the integral we are approximating could be over multi-million dimensional parameter spaces, and the posterior is likely non-Gaussiand and multi modal. We are also limited in the number of samples we can draw to approximate the posterior due to computational reasons. Thus we desire (i) typical points in the posterior, representing regions where there is a lot of mass and (ii) a diversity of points.\n",
    "\n",
    "Deep Ensembles have these two properties: through retraining a neural network multiple times with different initializations, unique low loss solutions in different basins of attractions can typically be found. Using methods like SGD, these points will center themselves in large basins of attractions.\n",
    "\n",
    "We can also view Deep Ensembles as forming a posterior approximated as point masses at different modes, combined with simple Monte Carlo integration: with this posterior, the Bayesian predictive distribution is $p(y|x, \\mathcal{D}) = \\frac{1}{J} \\sum_{j} p(y|x, w_j)$ where $w_j$ represent the different ensemble weights, which is exactly the standard deep ensemble procudure [1,3,8]\n",
    "![](posterior.png)\n",
    "\n",
    "\n",
    "**Figure 1.** ğ‘(ğ‘¦|ğ‘¥,ğ·)=âˆ«ğ‘(ğ‘¦|ğ‘¥,ğ‘¤)ğ‘(ğ‘¤|ğ·)ğ‘‘ğ‘¤. **Top**: ğ‘(ğ‘¤|ğ·), with representations from VI (orange) deep ensembles (blue), MultiSWAG (red). \n",
    "\n",
    "**Middle**: ğ‘(ğ‘¦|ğ‘¥,ğ‘¤) as a function of ğ‘¤ for a test input ğ‘¥. This function does not vary much within modes, but changes significantly between modes. \n",
    "\n",
    "**Bottom**: Distance between the true predictive distribution and the approximation, as a function of representing a posterior at an additional point ğ‘¤, assuming we have sampled the mode in dark green. There is more to be gained by exploring new basins, than continuing to explore the same basin.\n",
    "\n",
    "This idea is shown above in Figure 1 from [1]. The top panel is a multimodal posterior. The middle panel displays the predictive distribution $p(y|x, w)$ conditioned on paramaters w. Within a single basin, the predictive distribution does not change much, but between basins they are quite different. Therefor we would prefer to select different basins of attraction to provide a good approximation to the Bayesian Model Average integral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import push.bayes.ensemble\n",
    "\n",
    "# =============================================================================\n",
    "# Simple Dataset + Neural Network\n",
    "# =============================================================================\n",
    "\n",
    "class RandDataset(Dataset):\n",
    "    def __init__(self, D):\n",
    "        self.xs = torch.randn(128*10, D)\n",
    "        self.ys = torch.randn(128*10, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.xs[idx], self.ys[idx]\n",
    "\n",
    "\n",
    "class MiniNN(nn.Module):\n",
    "    def __init__(self, D):\n",
    "        super(MiniNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(D, D)\n",
    "        self.fc2 = nn.Linear(D, D)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class BiggerNN(nn.Module):\n",
    "    def __init__(self, n, D):\n",
    "        super(BiggerNN, self).__init__()\n",
    "        self.minis = []\n",
    "        self.n = n\n",
    "        for i in range(0, n):\n",
    "            self.minis += [MiniNN(D)]\n",
    "            self.add_module(\"mini_layer\"+str(i), self.minis[-1])\n",
    "        self.fc = nn.Linear(D, 1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for i in range(0, self.n):\n",
    "            x = self.minis[i](x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# L = 10\n",
    "# D = 20\n",
    "# dataset = RandDataset(D)\n",
    "# dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# epochs = 10\n",
    "# num_ensembles = 3\n",
    "# push.bayes.ensemble.train_deep_ensemble(\n",
    "#     dataloader,\n",
    "#     torch.nn.MSELoss(),\n",
    "#     epochs,\n",
    "#     BiggerNN, L, D,\n",
    "#     num_ensembles=num_ensembles\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "https://cims.nyu.edu/~andrewgw/deepensembles/\n",
    "\n",
    "[1] A.G. Wilson, P. Izmailov. Bayesian Deep Learning and a Probabilistic Perspective of Generalization. Advances in Neural Information Processing Systems, 2020.\n",
    "\n",
    "[2] P. Izmailov, S. Vikram, M.D. Hoffman, A.G. Wilson. What Are Bayesian Neural Network Posteriors Really Like? International Conference on Machine Learning, 2021.\n",
    "\n",
    "[3] A.G. Wilson. The Case for Bayesian Deep Learning. 2019.\n",
    "\n",
    "[4] A.G. Wilson. Thread on Deep Ensembles as Approximate Bayesian Inference. 2020.\n",
    "\n",
    "[5] A.G. Wilson. Examining Critiques in Bayesian Deep Learning. Video. April 2021.\n",
    "\n",
    "[6] C.E. Rasmussen and Z. Ghahramani. Bayesian Monte Carlo. Advances in Neural Information Processing Systems, 2003.\n",
    "\n",
    "[7] M. Osborne. Bayesian Gaussian processes for sequential prediction, optimisation and quadrature, PhD Thesis, 2010.\n",
    "\n",
    "[8] F. Gustafsson, M. Danelljan, T. Schon. Evaluating Scalable Bayesian Deep Learning Methods for Robust Computer Vision. CVPR Workshop, 2020.\n",
    "\n",
    "[9] P. Izmailov, P. Nicholson, S. Lotfi, A. G. Wilson. Dangers of Bayesian Model Averaging under Covariate Shift. Neural Information Processing Systems, 2021."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "push_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
